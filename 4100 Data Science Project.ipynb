{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Fingers == Trigger Fingers: A Look at Gun Violence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Acquisition and Cleaning Part 1: Data was gathered from one main source, gunviolencearchive.org. Each individual reporting of gun violence comes with sources to prove that it happened, as well as details on the number injured, killed, date, time, and place. The first data acquisition takes these variables, puts them into a dataframe then into csv, as well as finds the source and cleans the links so that they are easily accessible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tables(pd_dataframe):\n",
    "    # omit the hyperlink column that will be read as NA values\n",
    "    new_dataframe = pd_dataframe.loc[:,\"Incident ID\":\"# Injured\"]\n",
    "    # rename columns\n",
    "    new_dataframe = new_dataframe.rename(columns = {\"Incident ID\": \"ID\", \"Incident Date\": \"Date\", \n",
    "                                    \"State\": \"State\", \"City Or County\": \"City/County\", \n",
    "                                    \"Address\": \"Address\", \"# Killed\": \"Killed\", \n",
    "                                    \"# Injured\": \"Injured\"})\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(url, path):\n",
    "    response = requests.get(url)\n",
    "    with open(path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpages(soup, year):\n",
    "    last_webpage_href = soup.find('a', attrs={'title': \"Go to last page\"})\n",
    "    last_webpage_path = last_webpage_href.get('href')\n",
    "    number_of_other_pages = int(re.findall(r'%s(\\d+)'%\"page=\", last_webpage_path)[0])\n",
    "    if year in range(2014, 2016):\n",
    "        webpage_paths = ['/reports/mass-shootings/'+ str(year)] # initialize with the first page's path\n",
    "    else:\n",
    "        webpage_paths = ['/reports/mass-shooting?year='+ str(year)]\n",
    "    for page_number in range(1, number_of_other_pages + 1):\n",
    "        path = re.sub(str(number_of_other_pages), str(page_number), last_webpage_path)\n",
    "        webpage_paths.append(path)\n",
    "    return webpage_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_sources(soup):\n",
    "    news_hrefs = soup.findAll('a', attrs={'href': re.compile(\"^https://|^http://\")})\n",
    "    news_links = [tag.get('href') for tag in news_hrefs if tag.text == \"View Source\"] # get all sources listed on a page\n",
    "    return news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2014_mass_shootings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-8656a40ac52b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfirst_page_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.gunviolencearchive.org/reports/mass-shootings/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcsv_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_mass_shootings.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mthis_year_report\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mcleaned_report\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_tables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_year_report\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mannual_reports\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_report\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2014_mass_shootings.csv'"
     ]
    }
   ],
   "source": [
    "# get the report tables\n",
    "\n",
    "annual_reports = []\n",
    "for year in range(2014, 2020):\n",
    "    first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    csv_file = str(year) + \"_mass_shootings.csv\"\n",
    "    this_year_report = pd.read_csv(csv_file)\n",
    "    cleaned_report = transform_tables(this_year_report)\n",
    "    annual_reports.append(cleaned_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014 = annual_reports[0]\n",
    "ms_2015 = annual_reports[1]\n",
    "ms_2016 = annual_reports[2]\n",
    "ms_2017 = annual_reports[3]\n",
    "ms_2018 = annual_reports[4]\n",
    "ms_2019 = annual_reports[5]\n",
    "\n",
    "ms_2019[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save first pages html\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    if year in range(2014, 2016):\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    else:\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shooting?year=\" + str(year)\n",
    "    path = \"mass_shooting_html_\"+ str(year) \n",
    "    save_html(first_page_url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/reports/mass-shootings/2014',\n",
       "  '/reports/mass-shootings/2014?page=1',\n",
       "  '/reports/mass-shootings/2014?page=2',\n",
       "  '/reports/mass-shootings/2014?page=3',\n",
       "  '/reports/mass-shootings/2014?page=4',\n",
       "  '/reports/mass-shootings/2014?page=5',\n",
       "  '/reports/mass-shootings/2014?page=6',\n",
       "  '/reports/mass-shootings/2014?page=7',\n",
       "  '/reports/mass-shootings/2014?page=8',\n",
       "  '/reports/mass-shootings/2014?page=9',\n",
       "  '/reports/mass-shootings/2014?page=10'],\n",
       " ['/reports/mass-shootings/2015',\n",
       "  '/reports/mass-shootings/2015?page=1',\n",
       "  '/reports/mass-shootings/2015?page=2',\n",
       "  '/reports/mass-shootings/2015?page=3',\n",
       "  '/reports/mass-shootings/2015?page=4',\n",
       "  '/reports/mass-shootings/2015?page=5',\n",
       "  '/reports/mass-shootings/2015?page=6',\n",
       "  '/reports/mass-shootings/2015?page=7',\n",
       "  '/reports/mass-shootings/2015?page=8',\n",
       "  '/reports/mass-shootings/2015?page=9',\n",
       "  '/reports/mass-shootings/2015?page=10',\n",
       "  '/reports/mass-shootings/2015?page=11',\n",
       "  '/reports/mass-shootings/2015?page=12',\n",
       "  '/reports/mass-shootings/2015?page=13']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all pages paths\n",
    "\n",
    "web_pages_paths = []\n",
    "for year in range(2014, 2020):\n",
    "    path = \"mass_shooting_html_\"+ str(year)\n",
    "    soup = BeautifulSoup(open(path,'r'), 'html.parser')\n",
    "    web_pages_paths.append(get_webpages(soup, year)) # including the first\n",
    "\n",
    "web_pages_paths[:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mass_shooting_html_2014_page_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f541c4d12da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"mass_shooting_html_\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_page_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#         save_html(link, filename)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mthis_page_sources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_news_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msources_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0myear_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_page_sources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mass_shooting_html_2014_page_0'"
     ]
    }
   ],
   "source": [
    "sources_container = np.arange(2014, 2020, 1).tolist()\n",
    "for year_index in range(len(sources_container)):\n",
    "    year = 2014 + year_index\n",
    "    sources_container[year_index] = []\n",
    "    page_index = -1\n",
    "    for path in web_pages_paths[year_index]:\n",
    "        page_index += 1\n",
    "        link = \"https://www.gunviolencearchive.org\" + path\n",
    "        filename = \"mass_shooting_html_\"+ str(year) + \"_page_\" + str(page_index)\n",
    "#         save_html(link, filename)\n",
    "        soup = BeautifulSoup(open(filename,'r'), 'html.parser')\n",
    "        this_page_sources = get_news_sources(soup)\n",
    "        sources_container[year_index].append(this_page_sources) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_container[5][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nesting(nested_list):\n",
    "    return [i for j in nested_list for i in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-612a319fa9e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnews_2014\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_nesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnews_2015\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_nesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnews_2016\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_nesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnews_2017\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_nesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnews_2018\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_nesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msources_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-250a2bbba80d>\u001b[0m in \u001b[0;36mremove_nesting\u001b[1;34m(nested_list)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_nesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnested_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnested_list\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "news_2014 = remove_nesting(sources_container[0])\n",
    "news_2015 = remove_nesting(sources_container[1])\n",
    "news_2016 = remove_nesting(sources_container[2])\n",
    "news_2017 = remove_nesting(sources_container[3])\n",
    "news_2018 = remove_nesting(sources_container[4])\n",
    "news_2019 = remove_nesting(sources_container[5])\n",
    "news_2019[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014['Source'] = news_2014\n",
    "ms_2015['Source'] = news_2015\n",
    "ms_2016['Source'] = news_2016\n",
    "ms_2017['Source'] = news_2017\n",
    "ms_2018['Source'] = news_2018\n",
    "\n",
    "# ms_2019['Source'] = news_2019 \n",
    "# gives error since one row does not have a source listed directly\n",
    "\n",
    "index = news_2019.index(\"https://www.wcvb.com/article/6-people-shot-outside-of-roxbury-party-police-say/28306883\") # index of where it is supposed to be \n",
    "news_2019.insert(index, \"https://fox2now.com/2019/07/07/north-county-residents-on-edge-after-5-adults-found-dead-in-apartment/\")\n",
    "news_2019 = [news_2019[i] for i in range(len(news_2019)) if news_2019[i] != news_2019[i-1]] \n",
    "# in case of re-running the insert code and duplicating\n",
    "\n",
    "ms_2019['Source'] = news_2019 \n",
    "ms_2019[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014.to_csv(path_or_buf = \"complete_2014_dataset\")\n",
    "ms_2015.to_csv(path_or_buf = \"complete_2015_dataset\")\n",
    "ms_2016.to_csv(path_or_buf = \"complete_2016_dataset\")\n",
    "ms_2017.to_csv(path_or_buf = \"complete_2017_dataset\")\n",
    "ms_2018.to_csv(path_or_buf = \"complete_2018_dataset\")\n",
    "ms_2019.to_csv(path_or_buf = \"complete_2019_dataset\") # export as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.concat([ms_2014, ms_2015, ms_2016, ms_2017, ms_2018, ms_2019])\n",
    "print(len(merged_data))\n",
    "merged_data.to_csv(path_or_buf = \"complete_project_dataset1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Acquisition Part 2: The second step of the data acquisition process was to access the sources, and find the article text from each source. This allows us to create a second set of data in a text file for analysis of all words from these articles, \"articletext\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import urllib.request\n",
    "\n",
    "#reads in complete project data\n",
    "gunviolencedataset = pd.read_csv(\"complete_project_dataset\")\n",
    "sourceurl = gunviolencedataset[\"Source\"] #creates a series of just the source urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that takes the source url, goes to the url, reads in the text from that site. The function then writes the text from the \"p\" tag to a text file, so that it can be accessed afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsourcetext(urlseries):\n",
    "    #initialize beautifulsoup\n",
    "    soup = BeautifulSoup('''<html>  </html>''', 'html.parser') \n",
    "    timeout = 20 #creates a timeout variable w an int\n",
    "    socket.setdefaulttimeout(timeout) #uses timeout to set the socket timeout\n",
    "    textfile = open(\"articletext\", \"w+\",  encoding=\"utf-8\") #creates a writing file\n",
    "    for k in urlseries: #runs this loop for every entry in the series\n",
    "        try:\n",
    "            html = urllib.request.urlopen(url=k) #opens k website\n",
    "            html = html.read() #reads in website info\n",
    "            htmlfile = html.decode('utf-8') #decodes the info into a new file\n",
    "        #need to make exceptions for 404/403/etc\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        else: #what to do after try block works or doesn't work\n",
    "            singlesoup = BeautifulSoup(htmlfile, 'html.parser') #the variable that holds the data from the article\n",
    "            for n in singlesoup(\"p\"): #finds p tag (the main paragraph of the article)\n",
    "                textfile.write(n.get_text(strip=True)) #we only want the parts of the article that are from the main paragraph\n",
    "            soup.append(singlesoup) #adds the html file to the collective beautiful soup file\n",
    "    textfile.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block gives the \"allhtmlfiletext\" file and creates a beautifulsoup object of all the htmlfiles in the series. The purpose of creating the \"allhtmlfiletext\" is so that the getsourcetext() function defined above doesn't have to run everytime, it takes about 40-50 minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcesoup = getsourcetext(sourceurl) #makes sourcesoup into the entire beautiful soup, as well as finds the links of the source url series\n",
    "allhtmlfiles = sourcesoup.get_text(strip=True) #gets all of the written words out as a sting\n",
    "allhtmltxt = open(\"allhtmlfiletext\", \"w\", encoding = \"utf-8\") #creates a file\n",
    "allhtmltxt.write(allhtmlfiles) #adds this string to the file\n",
    "allhtmltxt.close() #closes file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analaysis Method Outlines: NLP and Sentiment Analysis Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Analysis: We wanted to use Natural Language Processing to inspect the relations of words in the articles we find, in the hopes that we can relate fequency of word use and the context that these fequently used words are in to the increase of gun violence in America. To start this the article text extracted must be tokenized (sorted into words), remove the words and punctuation that are too common to be useful, and assess the frequency of the cleaned words. Then ngrams are created of these words to assess the context that the frequent words come in. \n",
    "\n",
    "What remains to be done is connect these results to collective meaning about the articles, and how that relates to gun violence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "articletextfile = open(\"articletext\", encoding='utf-8')\n",
    "articlestring = articletextfile.read()\n",
    "articletextfile.close()\n",
    "\n",
    "#need to tokenize the article text\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "articletext = tokenizer.tokenize(articlestring)\n",
    "\n",
    "#need to remove stopwords and punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "capstopwords = [w.title() for w in stopwords]\n",
    "numbers = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"zero\"]\n",
    "stopwords.extend(capstopwords)\n",
    "stopwords.extend(numbers)\n",
    "articletext = [word.lower() for word in articletext if word not in stopwords]\n",
    "\n",
    "#finds most common words\n",
    "frequency_dist = FreqDist(articletext)\n",
    "keywordstuple = frequency_dist.most_common(10)\n",
    "keywords = []\n",
    "for wordtuple in keywordstuple:\n",
    "    keywords.append(wordtuple[0])\n",
    "print(keywordstuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram analysis of text\n",
    "bigrams = nltk.bigrams(articletext)\n",
    "trigrams = nltk.trigrams(articletext)\n",
    "bigramlist = list(bigrams)\n",
    "trigramlist = list(trigrams)\n",
    "#need to find the keywords in the ngrams\n",
    "keywordbigrams = [word_tuple for word_tuple in bigramlist if ((word_tuple[0] in keywords) | (word_tuple[1] in keywords))]\n",
    "keywordtrigrams = [word_tuple for word_tuple in trigramlist if ((word_tuple[0] in keywords) | (word_tuple[1] in keywords) | (word_tuple[2] in keywords))]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
