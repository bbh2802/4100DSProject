{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Fingers == Trigger Fingers: A Look at Gun Violence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Acquisition and Cleaning Part 1: Data was gathered from one main source, gunviolencearchive.org. Each individual reporting of gun violence comes with sources to prove that it happened, as well as details on the number injured, killed, date, time, and place. The first data acquisition takes these variables, puts them into a dataframe then into csv, as well as finds the source and cleans the links so that they are easily accessible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tables(pd_dataframe):\n",
    "    # omit the hyperlink column that will be read as NA values\n",
    "    new_dataframe = pd_dataframe.loc[:,\"Incident ID\":\"# Injured\"]\n",
    "    # rename columns\n",
    "    new_dataframe = new_dataframe.rename(columns = {\"Incident ID\": \"ID\", \"Incident Date\": \"Date\", \n",
    "                                    \"State\": \"State\", \"City Or County\": \"City/County\", \n",
    "                                    \"Address\": \"Address\", \"# Killed\": \"Killed\", \n",
    "                                    \"# Injured\": \"Injured\"})\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(url, path):\n",
    "    response = requests.get(url)\n",
    "    with open(path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpages(soup, year):\n",
    "    last_webpage_href = soup.find('a', attrs={'title': \"Go to last page\"})\n",
    "    last_webpage_path = last_webpage_href.get('href')\n",
    "    number_of_other_pages = int(re.findall(r'%s(\\d+)'%\"page=\", last_webpage_path)[0])\n",
    "    if year in range(2014, 2016):\n",
    "        webpage_paths = ['/reports/mass-shootings/'+ str(year)] # initialize with the first page's path\n",
    "    else:\n",
    "        webpage_paths = ['/reports/mass-shooting?year='+ str(year)]\n",
    "    for page_number in range(1, number_of_other_pages + 1):\n",
    "        path = re.sub(str(number_of_other_pages), str(page_number), last_webpage_path)\n",
    "        webpage_paths.append(path)\n",
    "    return webpage_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_sources(soup):\n",
    "    news_hrefs = soup.findAll('a', attrs={'href': re.compile(\"^https://|^http://\")})\n",
    "    news_links = [tag.get('href') for tag in news_hrefs if tag.text == \"View Source\"] # get all sources listed on a page\n",
    "    return news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nesting(nested_list):\n",
    "    return [i for j in nested_list for i in j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquisition and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pages paths\n",
    "\n",
    "web_pages_paths = []\n",
    "for year in range(2014, 2020):\n",
    "    path = \"mass_shooting_html_\"+ str(year)\n",
    "    soup = BeautifulSoup(open(path,'r'), 'html.parser')\n",
    "    web_pages_paths.append(get_webpages(soup, year)) # including the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save first pages html\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    if year in range(2014, 2016):\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    else:\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shooting?year=\" + str(year)\n",
    "    path = \"mass_shooting_html_\"+ str(year) \n",
    "    save_html(first_page_url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_container = np.arange(2014, 2020, 1).tolist()\n",
    "for year_index in range(len(sources_container)):\n",
    "    year = 2014 + year_index\n",
    "    sources_container[year_index] = []\n",
    "    page_index = -1\n",
    "    for path in web_pages_paths[year_index]:\n",
    "        page_index += 1\n",
    "        link = \"https://www.gunviolencearchive.org\" + path\n",
    "        filename = \"mass_shooting_html_\"+ str(year) + \"_page_\" + str(page_index)\n",
    "        #save_html(link, filename)\n",
    "        soup = BeautifulSoup(open(filename,'r'), 'html.parser')\n",
    "        this_page_sources = get_news_sources(soup)\n",
    "        sources_container[year_index].append(this_page_sources) \n",
    "sources_container[5][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_2014 = remove_nesting(sources_container[0])\n",
    "news_2015 = remove_nesting(sources_container[1])\n",
    "news_2016 = remove_nesting(sources_container[2])\n",
    "news_2017 = remove_nesting(sources_container[3])\n",
    "news_2018 = remove_nesting(sources_container[4])\n",
    "news_2019 = remove_nesting(sources_container[5])\n",
    "news_2019[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the report tables\n",
    "\n",
    "annual_reports = []\n",
    "for year in range(2014, 2020):\n",
    "    first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    csv_file = str(year) + \"_mass_shootings.csv\"\n",
    "    this_year_report = pd.read_csv(csv_file)\n",
    "    cleaned_report = transform_tables(this_year_report)\n",
    "    annual_reports.append(cleaned_report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014 = annual_reports[0]\n",
    "ms_2015 = annual_reports[1]\n",
    "ms_2016 = annual_reports[2]\n",
    "ms_2017 = annual_reports[3]\n",
    "ms_2018 = annual_reports[4]\n",
    "ms_2019 = annual_reports[5]\n",
    "\n",
    "ms_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014['Source'] = news_2014\n",
    "ms_2015['Source'] = news_2015\n",
    "ms_2016['Source'] = news_2016\n",
    "try:\n",
    "    ms_2017['Source'] = news_2017\n",
    "except Exception as e:\n",
    "    pass\n",
    "ms_2018['Source'] = news_2018\n",
    "\n",
    "# ms_2019['Source'] = news_2019 \n",
    "# gives error since one row does not have a source listed directly\n",
    "\n",
    "index = news_2019.index(\"https://www.wcvb.com/article/6-people-shot-outside-of-roxbury-party-police-say/28306883\") # index of where it is supposed to be \n",
    "news_2019.insert(index, \"https://fox2now.com/2019/07/07/north-county-residents-on-edge-after-5-adults-found-dead-in-apartment/\")\n",
    "news_2019 = [news_2019[i] for i in range(len(news_2019)) if news_2019[i] != news_2019[i-1]] \n",
    "# in case of re-running the insert code and duplicating\n",
    "\n",
    "ms_2019['Source'] = news_2019 \n",
    "ms_2019[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014.to_csv(path_or_buf = \"complete_2014_dataset\")\n",
    "ms_2015.to_csv(path_or_buf = \"complete_2015_dataset\")\n",
    "ms_2016.to_csv(path_or_buf = \"complete_2016_dataset\")\n",
    "ms_2017.to_csv(path_or_buf = \"complete_2017_dataset\")\n",
    "ms_2018.to_csv(path_or_buf = \"complete_2018_dataset\")\n",
    "ms_2019.to_csv(path_or_buf = \"complete_2019_dataset\") # export as csv files\n",
    "\n",
    "merged_data = pd.concat([ms_2014, ms_2015, ms_2016, ms_2017, ms_2018, ms_2019])\n",
    "print(len(merged_data))\n",
    "merged_data.to_csv(path_or_buf = \"complete_project_dataset1\") # export as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Acquisition Part 2: The second step of the data acquisition process was to access the sources, and find the article text from each source. This allows us to create a second set of data in a text file for analysis of all words from these articles, \"articletext\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import urllib.request\n",
    "\n",
    "#reads in complete project data\n",
    "gunviolencedataset = pd.read_csv(\"complete_project_dataset\")\n",
    "sourceurl = gunviolencedataset[\"Source\"] #creates a series of just the source urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing:\n",
    "ms_2014 = pd.read_csv(\"complete_2014_dataset\")\n",
    "print(ms_2014.index)\n",
    "urls = dataset['Source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing:\n",
    "def getsourcetext(urlseries):\n",
    "    #initialize beautifulsoup\n",
    "    soup = BeautifulSoup('''<html>  </html>''', 'html.parser') \n",
    "    timeout = 20 #creates a timeout variable w an int\n",
    "    socket.setdefaulttimeout(timeout) #uses timeout to set the socket timeout\n",
    "    dictionary = {}\n",
    "    for k in urlseries: #runs this loop for every entry in the series\n",
    "        try:\n",
    "            html = urllib.request.urlopen(url=k) #opens k website\n",
    "            html = html.read() #reads in website info\n",
    "            htmlfile = html.decode('utf-8') #decodes the info into a new file\n",
    "        #need to make exceptions for 404/403/etc\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        else: #what to do after try block works or doesn't work\n",
    "            singlesoup = BeautifulSoup(htmlfile, 'html.parser') #the variable that holds the data from the article\n",
    "            full_text = \"\"\n",
    "            for n in singlesoup(\"p\"): #finds p tag (the main paragraph of the article)\n",
    "                full_text += n.get_text(strip=True)#we only want the parts of the article that are from the main paragraph\n",
    "        dictionary[k] = full_text  \n",
    "    return dictionary\n",
    "\n",
    "article_text_dict = getsourcetext(urls) # takes 10-15 minutes just for these 2014 urls alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing:\n",
    "full_text_df = pd.DataFrame(list(article_text_dict.items()),\n",
    "                   columns=['Source', 'Text'])\n",
    "merged_2014_df = pd.merge(ms_2014, full_text_df, on = \"Source\")\n",
    "merged_2014_df.to_csv(path_or_buf = \"2014_dataset_with_text\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_2014_df # reduced dimension because a lot of urls did not work "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The function that takes the source url, goes to the url, reads in the text from that site. The function then writes the text from the \"p\" tag to a text file, so that it can be accessed afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsourcetext(urlseries):\n",
    "    #initialize beautifulsoup\n",
    "    soup = BeautifulSoup('''<html>  </html>''', 'html.parser') \n",
    "    timeout = 20 #creates a timeout variable w an int\n",
    "    socket.setdefaulttimeout(timeout) #uses timeout to set the socket timeout\n",
    "    textfile = open(\"articletext1\", \"w+\",  encoding=\"utf-8\") #creates a writing file\n",
    "    for k in urlseries: #runs this loop for every entry in the series\n",
    "        try:\n",
    "            html = urllib.request.urlopen(url=k) #opens k website\n",
    "            html = html.read() #reads in website info\n",
    "            htmlfile = html.decode('utf-8') #decodes the info into a new file\n",
    "        #need to make exceptions for 404/403/etc\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        else: #what to do after try block works or doesn't work\n",
    "            singlesoup = BeautifulSoup(htmlfile, 'html.parser') #the variable that holds the data from the article\n",
    "            for n in singlesoup(\"p\"): #finds p tag (the main paragraph of the article)\n",
    "                textfile.write(n.get_text(strip=True))#we only want the parts of the article that are from the main paragraph\n",
    "            soup.append(singlesoup) #adds the html file to the collective beautiful soup file\n",
    "    textfile.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block gives the \"allhtmlfiletext\" file and creates a beautifulsoup object of all the htmlfiles in the series. The purpose of creating the \"allhtmlfiletext\" is so that the getsourcetext() function defined above doesn't have to run everytime, it takes about 40-50 minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcesoup = getsourcetext(sourceurl) #makes sourcesoup into the entire beautiful soup, as well as finds the links of the source url series\n",
    "allhtmlfiles = sourcesoup.get_text(strip=True) #gets all of the written words out as a sting\n",
    "allhtmltxt = open(\"allhtmlfiletext\", \"w\", encoding = \"utf-8\") #creates a file\n",
    "allhtmltxt.write(allhtmlfiles) #adds this string to the file\n",
    "allhtmltxt.close() #closes file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Analysis: After obtaining and cleaning the data, we wanted to look at the statistics and visualization of the data gathered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('complete_project_dataset',index_col=0)\n",
    "\n",
    "data_2014 = data[0:269]\n",
    "data_2015 = data[270:604]\n",
    "data_2016 = data[605:986]\n",
    "data_2017 = data[987:1332]\n",
    "data_2018 = data[1333:1668]\n",
    "data_2019 = data[1669:2085]\n",
    "\n",
    "data_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "num_killed = []\n",
    "num_injured = []\n",
    "\n",
    "for row in range(len(data)):\n",
    "    x = datetime.datetime.strptime(data.iloc[row,1],\"%B %d, %Y\")\n",
    "    if x in dates:\n",
    "        num_killed[len(num_killed)-1]+= data.iloc[row,5]\n",
    "        num_injured[len(num_injured)-1]+= data.iloc[row,6]\n",
    "    else:\n",
    "        dates.append(x)\n",
    "        num_killed.append(data.iloc[row,5])\n",
    "        num_injured.append(data.iloc[row,6])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates, num_injured, label = \"People Injured\")\n",
    "plt.plot(dates, num_killed, label = \"People Killed\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Dates')\n",
    "# naming the y axis\n",
    "plt.ylabel('Number of People')\n",
    "# giving a title to my graph\n",
    "plt.title('Number of People Killed or Injured by Guns')\n",
    "  \n",
    "# show a legend on the plot\n",
    "plt.legend()\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'statistic' : ['mean','median','mode','std','min','max'], \n",
    "     'Number Killed' : [np.mean(num_killed),np.median(num_killed),statistics.mode(num_killed),np.std(num_killed),min(num_killed),max(num_killed)],\n",
    "    'Number Injured' : [np.mean(num_injured),np.median(num_injured),statistics.mode(num_injured),np.std(num_injured),min(num_injured),max(num_injured)]\n",
    "    }\n",
    "\n",
    "stats_of_the_data = pd.DataFrame(data = d)\n",
    "print(stats_of_the_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analaysis Method Outlines: NLP and Sentiment Analysis Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Analysis: We wanted to use Natural Language Processing to inspect the relations of words in the articles we find, in the hopes that we can relate fequency of word use and the context that these fequently used words are in to the increase of gun violence in America. To start this the article text extracted must be tokenized (sorted into words), remove the words and punctuation that are too common to be useful, and assess the frequency of the cleaned words. Then ngrams are created of these words to assess the context that the frequent words come in. \n",
    "\n",
    "What remains to be done is connect these results to collective meaning about the articles, and how that relates to gun violence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# articletextfile = open(\"articletext\", encoding='utf-8')\n",
    "# articlestring = articletextfile.read()\n",
    "# articletextfile.close()\n",
    "\n",
    "# #need to tokenize the article text\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# articletext = tokenizer.tokenize(articlestring)\n",
    "\n",
    "#need to remove stopwords and punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "capstopwords = [w.title() for w in stopwords]\n",
    "numbers = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"zero\"]\n",
    "stopwords.extend(capstopwords)\n",
    "stopwords.extend(numbers)\n",
    "# articletext = [word.lower() for word in articletext if word not in stopwords]\n",
    "\n",
    "#finds most common words\n",
    "# frequency_dist = FreqDist(articletext)\n",
    "# keywordstuple = frequency_dist.most_common(10)\n",
    "# keywords = []\n",
    "# for wordtuple in keywordstuple:\n",
    "#     keywords.append(wordtuple[0])\n",
    "# print(keywordstuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing:\n",
    "keywordstuple_list = []\n",
    "for article in merged_2014_df['Text']:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    articletext = tokenizer.tokenize(article)\n",
    "    articletext = [word.lower() for word in articletext if word not in stopwords]\n",
    "    frequency_dist = FreqDist(articletext)\n",
    "    keywordstuple = frequency_dist.most_common(10)\n",
    "    keywordstuple_list.append(list(keywordstuple))\n",
    "\n",
    "new_df[\"keywordstuple\"] = keywordstuple_list\n",
    "new_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram analysis of text\n",
    "bigrams = nltk.bigrams(articletext)\n",
    "trigrams = nltk.trigrams(articletext)\n",
    "bigramlist = list(bigrams)\n",
    "trigramlist = list(trigrams)\n",
    "#need to find the keywords in the ngrams\n",
    "keywordbigrams = [word_tuple for word_tuple in bigramlist if ((word_tuple[0] in keywords) | (word_tuple[1] in keywords))]\n",
    "keywordtrigrams = [word_tuple for word_tuple in trigramlist if ((word_tuple[0] in keywords) | (word_tuple[1] in keywords) | (word_tuple[2] in keywords))]\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis: Our second kind of analysis for the article is going to be sentiment analysis classification. We created a classification model that trains on a library of tweets that are rated from 0 through 4 on a negative-positive scale. The tweet is vectorized into features and its rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "except Exception as e:\n",
    "    pass\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and trimming twitter sentiment dataset\n",
    "\n",
    "Insert header, remove numbers, usernames, and NO_QUERY\n",
    "\n",
    "Move first row that became header down to data, add headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding = \"ISO-8859-1\",names=[\"score\",\"id\",\"datetime\",\"NO_QUERY\",\"usernames\",\"tweet\"])\n",
    "\n",
    "df = df.drop(['id','NO_QUERY','usernames'],axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking count values, dataset listed from 0 = negative to 4 = positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0:20000, :]\n",
    "print(df.shape)\n",
    "print(df['score'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up links, @users, hastags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def cleanup(text):\n",
    "    clean = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(text)).split()\n",
    "    tokens = []\n",
    "    for token in clean:\n",
    "        if token not in stopwords:\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(lambda x: cleanup(x))\n",
    "\n",
    "df['score'] = df['score'].replace([0,4],['neg','pos']) \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuple(x):\n",
    "    subset = x[['tweet','score']]\n",
    "    tuples = [tuple(i) for i in subset.to_numpy()]\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = to_tuple(df)\n",
    "documents[0] # tuple of tokens and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_master(x):\n",
    "#     master_list = []\n",
    "#     for i in range(len(x)):\n",
    "#         master_list += x['tweet'][i] \n",
    "#     return master_list\n",
    "\n",
    "words = remove_nesting(df['tweet'])\n",
    "\n",
    "word_features = nltk.FreqDist(w.lower() for w in words).most_common(2000)\n",
    "word_features =  [word_tuple[0] for word_tuple in word_features]\n",
    "word_features[0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):    \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains('+ word +')'] = (word in document_words) \n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents] \n",
    "featuresets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words as feature vectors have been computed, this then establishes the training and test sets. Then runs the classification model on the test set. \n",
    "\n",
    "What remains to be done here is test the model on the article data which also needs to be turned into feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
