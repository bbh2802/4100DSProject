{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Fingers == Trigger Fingers: A Look at Gun Violence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Acquisition and Cleaning Part 1: Data was gathered from one main source, gunviolencearchive.org. Each individual reporting of gun violence comes with sources to prove that it happened, as well as details on the number injured, killed, date, time, and place. The first data acquisition takes these variables, puts them into a dataframe then into csv, as well as finds the source and cleans the links so that they are easily accessible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tables(pd_dataframe):\n",
    "    # omit the hyperlink column that will be read as NA values\n",
    "    new_dataframe = pd_dataframe.loc[:,\"Incident ID\":\"# Injured\"]\n",
    "    # rename columns\n",
    "    new_dataframe = new_dataframe.rename(columns = {\"Incident ID\": \"ID\", \"Incident Date\": \"Date\", \n",
    "                                    \"State\": \"State\", \"City Or County\": \"City/County\", \n",
    "                                    \"Address\": \"Address\", \"# Killed\": \"Killed\", \n",
    "                                    \"# Injured\": \"Injured\"})\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(url, path):\n",
    "    response = requests.get(url)\n",
    "    with open(path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpages(soup, year):\n",
    "    last_webpage_href = soup.find('a', attrs={'title': \"Go to last page\"})\n",
    "    last_webpage_path = last_webpage_href.get('href')\n",
    "    number_of_other_pages = int(re.findall(r'%s(\\d+)'%\"page=\", last_webpage_path)[0])\n",
    "    if year in range(2014, 2016):\n",
    "        webpage_paths = ['/reports/mass-shootings/'+ str(year)] # initialize with the first page's path\n",
    "    else:\n",
    "        webpage_paths = ['/reports/mass-shooting?year='+ str(year)]\n",
    "    for page_number in range(1, number_of_other_pages + 1):\n",
    "        path = re.sub(str(number_of_other_pages), str(page_number), last_webpage_path)\n",
    "        webpage_paths.append(path)\n",
    "    return webpage_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_sources(soup):\n",
    "    news_hrefs = soup.findAll('a', attrs={'href': re.compile(\"^https://|^http://\")})\n",
    "    news_links = [tag.get('href') for tag in news_hrefs if tag.text == \"View Source\"] # get all sources listed on a page\n",
    "    return news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nesting(nested_list):\n",
    "    return [i for j in nested_list for i in j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquisition and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pages paths\n",
    "\n",
    "web_pages_paths = []\n",
    "for year in range(2014, 2020):\n",
    "    path = \"mass_shooting_html_\"+ str(year)\n",
    "    soup = BeautifulSoup(open(path,'r'), 'html.parser')\n",
    "    web_pages_paths.append(get_webpages(soup, year)) # including the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save first pages html\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    if year in range(2014, 2016):\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    else:\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shooting?year=\" + str(year)\n",
    "    path = \"mass_shooting_html_\"+ str(year) \n",
    "    save_html(first_page_url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_container = np.arange(2014, 2020, 1).tolist()\n",
    "for year_index in range(len(sources_container)):\n",
    "    year = 2014 + year_index\n",
    "    sources_container[year_index] = []\n",
    "    page_index = -1\n",
    "    for path in web_pages_paths[year_index]:\n",
    "        page_index += 1\n",
    "        link = \"https://www.gunviolencearchive.org\" + path\n",
    "        filename = \"mass_shooting_html_\"+ str(year) + \"_page_\" + str(page_index)\n",
    "        #save_html(link, filename)\n",
    "        soup = BeautifulSoup(open(filename,'r'), 'html.parser')\n",
    "        this_page_sources = get_news_sources(soup)\n",
    "        sources_container[year_index].append(this_page_sources) \n",
    "sources_container[5][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_2014 = remove_nesting(sources_container[0])\n",
    "news_2015 = remove_nesting(sources_container[1])\n",
    "news_2016 = remove_nesting(sources_container[2])\n",
    "news_2017 = remove_nesting(sources_container[3])\n",
    "news_2018 = remove_nesting(sources_container[4])\n",
    "news_2019 = remove_nesting(sources_container[5])\n",
    "news_2019[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the report tables\n",
    "\n",
    "annual_reports = []\n",
    "for year in range(2014, 2020):\n",
    "    first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    csv_file = str(year) + \"_mass_shootings.csv\"\n",
    "    this_year_report = pd.read_csv(csv_file)\n",
    "    cleaned_report = transform_tables(this_year_report)\n",
    "    annual_reports.append(cleaned_report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014 = annual_reports[0]\n",
    "ms_2015 = annual_reports[1]\n",
    "ms_2016 = annual_reports[2]\n",
    "ms_2017 = annual_reports[3]\n",
    "ms_2018 = annual_reports[4]\n",
    "ms_2019 = annual_reports[5]\n",
    "\n",
    "ms_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014['Source'] = news_2014\n",
    "ms_2015['Source'] = news_2015\n",
    "ms_2016['Source'] = news_2016\n",
    "try:\n",
    "    ms_2017['Source'] = news_2017\n",
    "except Exception as e:\n",
    "    pass\n",
    "ms_2018['Source'] = news_2018\n",
    "\n",
    "# ms_2019['Source'] = news_2019 \n",
    "# gives error since one row does not have a source listed directly\n",
    "\n",
    "index = news_2019.index(\"https://www.wcvb.com/article/6-people-shot-outside-of-roxbury-party-police-say/28306883\") # index of where it is supposed to be \n",
    "news_2019.insert(index, \"https://fox2now.com/2019/07/07/north-county-residents-on-edge-after-5-adults-found-dead-in-apartment/\")\n",
    "news_2019 = [news_2019[i] for i in range(len(news_2019)) if news_2019[i] != news_2019[i-1]] \n",
    "# in case of re-running the insert code and duplicating\n",
    "\n",
    "ms_2019['Source'] = news_2019 \n",
    "ms_2019[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014.to_csv(path_or_buf = \"complete_2014_dataset\")\n",
    "ms_2015.to_csv(path_or_buf = \"complete_2015_dataset\")\n",
    "ms_2016.to_csv(path_or_buf = \"complete_2016_dataset\")\n",
    "ms_2017.to_csv(path_or_buf = \"complete_2017_dataset\")\n",
    "ms_2018.to_csv(path_or_buf = \"complete_2018_dataset\")\n",
    "ms_2019.to_csv(path_or_buf = \"complete_2019_dataset\") # export as csv files\n",
    "\n",
    "merged_data = pd.concat([ms_2014, ms_2015, ms_2016, ms_2017, ms_2018, ms_2019])\n",
    "print(len(merged_data))\n",
    "merged_data.to_csv(path_or_buf = \"complete_project_dataset1\") # export as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Acquisition Part 2: The second step of the data acquisition process was to access the sources, and find the article text from each source. This allows us to create a second set of data in a text file for analysis of all words from these articles, \"articletext\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import urllib.request\n",
    "\n",
    "#reads in complete project data\n",
    "gunviolencedataset = pd.read_csv(\"complete_project_dataset\")\n",
    "sourceurl = gunviolencedataset[\"Source\"] #creates a series of just the source urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing:\n",
    "ms_2014 = pd.read_csv(\"complete_2014_dataset\")\n",
    "print(ms_2014.index)\n",
    "urls = dataset['Source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing:\n",
    "def getsourcetext(urlseries):\n",
    "    #initialize beautifulsoup\n",
    "    soup = BeautifulSoup('''<html>  </html>''', 'html.parser') \n",
    "    timeout = 20 #creates a timeout variable w an int\n",
    "    socket.setdefaulttimeout(timeout) #uses timeout to set the socket timeout\n",
    "    dictionary = {}\n",
    "    for k in urlseries: #runs this loop for every entry in the series\n",
    "        try:\n",
    "            html = urllib.request.urlopen(url=k) #opens k website\n",
    "            html = html.read() #reads in website info\n",
    "            htmlfile = html.decode('utf-8') #decodes the info into a new file\n",
    "        #need to make exceptions for 404/403/etc\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        else: #what to do after try block works or doesn't work\n",
    "            singlesoup = BeautifulSoup(htmlfile, 'html.parser') #the variable that holds the data from the article\n",
    "            full_text = \"\"\n",
    "            for n in singlesoup(\"p\"): #finds p tag (the main paragraph of the article)\n",
    "                full_text += n.get_text(strip=True)#we only want the parts of the article that are from the main paragraph\n",
    "        dictionary[k] = full_text  \n",
    "    return dictionary\n",
    "\n",
    "text_dict_of_1000 = getsourcetext(sourceurl[:1000]) # it takes 1 hour to work the first 1000 urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing:\n",
    "# full_text_df = pd.DataFrame(list(article_text_dict.items()),\n",
    "#                    columns=['Source', 'Text'])\n",
    "# merged_2014_df = pd.merge(ms_2014, full_text_df, on = \"Source\")\n",
    "# merged_2014_df.to_csv(path_or_buf = \"2014_dataset_with_text\") \n",
    "\n",
    "full_text_df = pd.DataFrame(list(text_dict_of_1000.items()),\n",
    "                   columns=['Source', 'Text'])\n",
    "merged_1000_df = pd.merge(gunviolencedataset, full_text_df, on = \"Source\")\n",
    "merged_1000_df.to_csv(path_or_buf = \"first_1000_dataset_with_text\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>State</th>\n",
       "      <th>City/County</th>\n",
       "      <th>Address</th>\n",
       "      <th>Killed</th>\n",
       "      <th>Injured</th>\n",
       "      <th>Source</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>269679</td>\n",
       "      <td>December 27, 2014</td>\n",
       "      <td>California</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>8800 block of South Figueroa Street</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>http://ktla.com/2014/12/27/south-l-a-shooting-...</td>\n",
       "      <td>KTLAby:John A. Moreno(Credit: KTLA)A shooting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>268598</td>\n",
       "      <td>December 24, 2014</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>Saint Louis</td>\n",
       "      <td>18th and Pine</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.stltoday.com/news/local/crime-and-c...</td>\n",
       "      <td>ST. LOUIS•   Police on Thursday released more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>268282</td>\n",
       "      <td>December 22, 2014</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Detroit</td>\n",
       "      <td>Charlevoix and Philip</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.wxyz.com/news/region/wayne-county/f...</td>\n",
       "      <td>MenuTwo families ripped devastated by the murd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>282186</td>\n",
       "      <td>December 22, 2014</td>\n",
       "      <td>New York</td>\n",
       "      <td>Webster</td>\n",
       "      <td>191 Lake Rd</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-22544...</td>\n",
       "      <td>ByHelen PowPublished:18:48 EDT, 28 December 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>267721</td>\n",
       "      <td>December 22, 2014</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>5700 block of South Green Street</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.dnainfo.com/chicago/20141223/engle...</td>\n",
       "      <td>The DNAinfo archives brought to you by WNYC.Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>7</td>\n",
       "      <td>1014971</td>\n",
       "      <td>December 23, 2017</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>Eunice</td>\n",
       "      <td>100 block of City Avenue</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>http://klfy.com/2017/12/29/local-rapper-charge...</td>\n",
       "      <td>KLFYby:StaffEUNICE, La. (KLFY)-Acting on a war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>8</td>\n",
       "      <td>1013377</td>\n",
       "      <td>December 23, 2017</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>3rd Ave N and 9th St N</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.al.com/news/birmingham/index.ssf/20...</td>\n",
       "      <td>Four men were injured early Saturday morning i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>9</td>\n",
       "      <td>1011953</td>\n",
       "      <td>December 21, 2017</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>6600 block of Dorel St</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.philly.com/news/jabir-kennedy-self...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>12</td>\n",
       "      <td>1008310</td>\n",
       "      <td>December 15, 2017</td>\n",
       "      <td>California</td>\n",
       "      <td>Pacoima</td>\n",
       "      <td>13041 Jouett Street</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>http://losangeles.cbslocal.com/2017/12/15/4-sh...</td>\n",
       "      <td>PACOIMA (CBSLA) —Five people have been taken t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>13</td>\n",
       "      <td>1007054</td>\n",
       "      <td>December 13, 2017</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>200 block of E Barthman Ave</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.10tv.com/article/1-dead-3-wounded-...</td>\n",
       "      <td>COLUMBUS, Ohio– One person was killed and thre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0       ID               Date         State   City/County  \\\n",
       "0             1   269679  December 27, 2014    California   Los Angeles   \n",
       "1             4   268598  December 24, 2014      Missouri   Saint Louis   \n",
       "2             6   268282  December 22, 2014      Michigan       Detroit   \n",
       "3             7   282186  December 22, 2014      New York       Webster   \n",
       "4             8   267721  December 22, 2014      Illinois       Chicago   \n",
       "..          ...      ...                ...           ...           ...   \n",
       "578           7  1014971  December 23, 2017     Louisiana        Eunice   \n",
       "579           8  1013377  December 23, 2017       Alabama    Birmingham   \n",
       "580           9  1011953  December 21, 2017  Pennsylvania  Philadelphia   \n",
       "581          12  1008310  December 15, 2017    California       Pacoima   \n",
       "582          13  1007054  December 13, 2017          Ohio      Columbus   \n",
       "\n",
       "                                 Address  Killed  Injured  \\\n",
       "0    8800 block of South Figueroa Street       1        3   \n",
       "1                          18th and Pine       1        3   \n",
       "2                  Charlevoix and Philip       1        3   \n",
       "3                            191 Lake Rd       4        2   \n",
       "4       5700 block of South Green Street       0        5   \n",
       "..                                   ...     ...      ...   \n",
       "578             100 block of City Avenue       1        3   \n",
       "579               3rd Ave N and 9th St N       0        4   \n",
       "580               6600 block of Dorel St       1        3   \n",
       "581                  13041 Jouett Street       0        5   \n",
       "582          200 block of E Barthman Ave       1        3   \n",
       "\n",
       "                                                Source  \\\n",
       "0    http://ktla.com/2014/12/27/south-l-a-shooting-...   \n",
       "1    http://www.stltoday.com/news/local/crime-and-c...   \n",
       "2    http://www.wxyz.com/news/region/wayne-county/f...   \n",
       "3    https://www.dailymail.co.uk/news/article-22544...   \n",
       "4    https://www.dnainfo.com/chicago/20141223/engle...   \n",
       "..                                                 ...   \n",
       "578  http://klfy.com/2017/12/29/local-rapper-charge...   \n",
       "579  http://www.al.com/news/birmingham/index.ssf/20...   \n",
       "580  https://www.philly.com/news/jabir-kennedy-self...   \n",
       "581  http://losangeles.cbslocal.com/2017/12/15/4-sh...   \n",
       "582  https://www.10tv.com/article/1-dead-3-wounded-...   \n",
       "\n",
       "                                                  Text  \n",
       "0    KTLAby:John A. Moreno(Credit: KTLA)A shooting ...  \n",
       "1    ST. LOUIS•   Police on Thursday released more ...  \n",
       "2    MenuTwo families ripped devastated by the murd...  \n",
       "3    ByHelen PowPublished:18:48 EDT, 28 December 20...  \n",
       "4    The DNAinfo archives brought to you by WNYC.Re...  \n",
       "..                                                 ...  \n",
       "578  KLFYby:StaffEUNICE, La. (KLFY)-Acting on a war...  \n",
       "579  Four men were injured early Saturday morning i...  \n",
       "580                                                     \n",
       "581  PACOIMA (CBSLA) —Five people have been taken t...  \n",
       "582  COLUMBUS, Ohio– One person was killed and thre...  \n",
       "\n",
       "[583 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_1000_df # reduced dimension to 583 rows because a lot of urls did not work "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The function that takes the source url, goes to the url, reads in the text from that site. The function then writes the text from the \"p\" tag to a text file, so that it can be accessed afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsourcetext(urlseries):\n",
    "    #initialize beautifulsoup\n",
    "    soup = BeautifulSoup('''<html>  </html>''', 'html.parser') \n",
    "    timeout = 20 #creates a timeout variable w an int\n",
    "    socket.setdefaulttimeout(timeout) #uses timeout to set the socket timeout\n",
    "    textfile = open(\"articletext1\", \"w+\",  encoding=\"utf-8\") #creates a writing file\n",
    "    for k in urlseries: #runs this loop for every entry in the series\n",
    "        try:\n",
    "            html = urllib.request.urlopen(url=k) #opens k website\n",
    "            html = html.read() #reads in website info\n",
    "            htmlfile = html.decode('utf-8') #decodes the info into a new file\n",
    "        #need to make exceptions for 404/403/etc\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        else: #what to do after try block works or doesn't work\n",
    "            singlesoup = BeautifulSoup(htmlfile, 'html.parser') #the variable that holds the data from the article\n",
    "            for n in singlesoup(\"p\"): #finds p tag (the main paragraph of the article)\n",
    "                textfile.write(n.get_text(strip=True))#we only want the parts of the article that are from the main paragraph\n",
    "            soup.append(singlesoup) #adds the html file to the collective beautiful soup file\n",
    "    textfile.close()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block gives the \"allhtmlfiletext\" file and creates a beautifulsoup object of all the htmlfiles in the series. The purpose of creating the \"allhtmlfiletext\" is so that the getsourcetext() function defined above doesn't have to run everytime, it takes about 40-50 minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcesoup = getsourcetext(sourceurl) #makes sourcesoup into the entire beautiful soup, as well as finds the links of the source url series\n",
    "allhtmlfiles = sourcesoup.get_text(strip=True) #gets all of the written words out as a sting\n",
    "allhtmltxt = open(\"allhtmlfiletext\", \"w\", encoding = \"utf-8\") #creates a file\n",
    "allhtmltxt.write(allhtmlfiles) #adds this string to the file\n",
    "allhtmltxt.close() #closes file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Analysis: After obtaining and cleaning the data, we wanted to look at the statistics and visualization of the data gathered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('complete_project_dataset',index_col=0)\n",
    "\n",
    "data_2014 = data[0:269]\n",
    "data_2015 = data[270:604]\n",
    "data_2016 = data[605:986]\n",
    "data_2017 = data[987:1332]\n",
    "data_2018 = data[1333:1668]\n",
    "data_2019 = data[1669:2085]\n",
    "\n",
    "data_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "num_killed = []\n",
    "num_injured = []\n",
    "\n",
    "for row in range(len(data)):\n",
    "    x = datetime.datetime.strptime(data.iloc[row,1],\"%B %d, %Y\")\n",
    "    if x in dates:\n",
    "        num_killed[len(num_killed)-1]+= data.iloc[row,5]\n",
    "        num_injured[len(num_injured)-1]+= data.iloc[row,6]\n",
    "    else:\n",
    "        dates.append(x)\n",
    "        num_killed.append(data.iloc[row,5])\n",
    "        num_injured.append(data.iloc[row,6])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates, num_injured, label = \"People Injured\")\n",
    "plt.plot(dates, num_killed, label = \"People Killed\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Dates')\n",
    "# naming the y axis\n",
    "plt.ylabel('Number of People')\n",
    "# giving a title to my graph\n",
    "plt.title('Number of People Killed or Injured by Guns')\n",
    "  \n",
    "# show a legend on the plot\n",
    "plt.legend()\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'statistic' : ['mean','median','mode','std','min','max'], \n",
    "     'Number Killed' : [np.mean(num_killed),np.median(num_killed),statistics.mode(num_killed),np.std(num_killed),min(num_killed),max(num_killed)],\n",
    "    'Number Injured' : [np.mean(num_injured),np.median(num_injured),statistics.mode(num_injured),np.std(num_injured),min(num_injured),max(num_injured)]\n",
    "    }\n",
    "\n",
    "stats_of_the_data = pd.DataFrame(data = d)\n",
    "print(stats_of_the_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analaysis Method Outlines: NLP and Sentiment Analysis Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Analysis: We wanted to use Natural Language Processing to inspect the relations of words in the articles we find, in the hopes that we can relate fequency of word use and the context that these fequently used words are in to the increase of gun violence in America. To start this the article text extracted must be tokenized (sorted into words), remove the words and punctuation that are too common to be useful, and assess the frequency of the cleaned words. Then ngrams are created of these words to assess the context that the frequent words come in. \n",
    "\n",
    "What remains to be done is connect these results to collective meaning about the articles, and how that relates to gun violence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# articletextfile = open(\"articletext\", encoding='utf-8')\n",
    "# articlestring = articletextfile.read()\n",
    "# articletextfile.close()\n",
    "\n",
    "# #need to tokenize the article text\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# articletext = tokenizer.tokenize(articlestring)\n",
    "\n",
    "#need to remove stopwords and punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "capstopwords = [w.title() for w in stopwords]\n",
    "numbers = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"zero\"]\n",
    "stopwords.extend(capstopwords)\n",
    "stopwords.extend(numbers)\n",
    "# articletext = [word.lower() for word in articletext if word not in stopwords]\n",
    "\n",
    "#finds most common words\n",
    "# frequency_dist = FreqDist(articletext)\n",
    "# keywordstuple = frequency_dist.most_common(10)\n",
    "# keywords = []\n",
    "# for wordtuple in keywordstuple:\n",
    "#     keywords.append(wordtuple[0])\n",
    "# print(keywordstuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing:\n",
    "keywordstuple_list = []\n",
    "for article in merged_2014_df['Text']:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    articletext = tokenizer.tokenize(article)\n",
    "    articletext = [word.lower() for word in articletext if word not in stopwords]\n",
    "    frequency_dist = FreqDist(articletext)\n",
    "    keywordstuple = frequency_dist.most_common(10)\n",
    "    keywordstuple_list.append(list(keywordstuple))\n",
    "\n",
    "new_df[\"keywordstuple\"] = keywordstuple_list\n",
    "new_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram analysis of text\n",
    "bigrams = nltk.bigrams(articletext)\n",
    "trigrams = nltk.trigrams(articletext)\n",
    "bigramlist = list(bigrams)\n",
    "trigramlist = list(trigrams)\n",
    "#need to find the keywords in the ngrams\n",
    "keywordbigrams = [word_tuple for word_tuple in bigramlist if ((word_tuple[0] in keywords) | (word_tuple[1] in keywords))]\n",
    "keywordtrigrams = [word_tuple for word_tuple in trigramlist if ((word_tuple[0] in keywords) | (word_tuple[1] in keywords) | (word_tuple[2] in keywords))]\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis: Our second kind of analysis for the article is going to be sentiment analysis classification. We created a classification model that trains on a library of tweets that are rated from 0 through 4 on a negative-positive scale. The tweet is vectorized into features and its rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "except Exception as e:\n",
    "    pass\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and trimming twitter sentiment dataset\n",
    "\n",
    "Insert header, remove numbers, usernames, and NO_QUERY\n",
    "\n",
    "Move first row that became header down to data, add headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding = \"ISO-8859-1\",names=[\"score\",\"id\",\"datetime\",\"NO_QUERY\",\"usernames\",\"tweet\"])\n",
    "\n",
    "df = df.drop(['id','NO_QUERY','usernames'],axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking count values, dataset listed from 0 = negative to 4 = positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0:20000, :]\n",
    "print(df.shape)\n",
    "print(df['score'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up links, @users, hastags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def cleanup(text):\n",
    "    clean = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(text)).split()\n",
    "    tokens = []\n",
    "    for token in clean:\n",
    "        if token not in stopwords:\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(lambda x: cleanup(x))\n",
    "\n",
    "df['score'] = df['score'].replace([0,4],['neg','pos']) \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuple(x):\n",
    "    subset = x[['tweet','score']]\n",
    "    tuples = [tuple(i) for i in subset.to_numpy()]\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = to_tuple(df)\n",
    "documents[0] # tuple of tokens and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_master(x):\n",
    "#     master_list = []\n",
    "#     for i in range(len(x)):\n",
    "#         master_list += x['tweet'][i] \n",
    "#     return master_list\n",
    "\n",
    "words = remove_nesting(df['tweet'])\n",
    "\n",
    "word_features = nltk.FreqDist(w.lower() for w in words).most_common(2000)\n",
    "word_features =  [word_tuple[0] for word_tuple in word_features]\n",
    "word_features[0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):    \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains('+ word +')'] = (word in document_words) \n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents] \n",
    "featuresets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words as feature vectors have been computed, this then establishes the training and test sets. Then runs the classification model on the test set. \n",
    "\n",
    "What remains to be done here is test the model on the article data which also needs to be turned into feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
