{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project\n",
    "\n",
    "# Data Acquisition (revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tables(pd_dataframe):\n",
    "    # omit the hyperlink column that will be read as NA values\n",
    "    new_dataframe = pd_dataframe.loc[:,\"Incident ID\":\"# Injured\"]\n",
    "    # rename columns\n",
    "    new_dataframe = new_dataframe.rename(columns = {\"Incident ID\": \"ID\", \"Incident Date\": \"Date\", \n",
    "                                    \"State\": \"State\", \"City Or County\": \"City/County\", \n",
    "                                    \"Address\": \"Address\", \"# Killed\": \"Killed\", \n",
    "                                    \"# Injured\": \"Injured\"})\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(url, path):\n",
    "    response = requests.get(url)\n",
    "    with open(path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpages(soup, year):\n",
    "    last_webpage_href = soup.find('a', attrs={'title': \"Go to last page\"})\n",
    "    last_webpage_path = last_webpage_href.get('href')\n",
    "    number_of_other_pages = int(re.findall(r'%s(\\d+)'%\"page=\", last_webpage_path)[0])\n",
    "    if year in range(2014, 2016):\n",
    "        webpage_paths = ['/reports/mass-shootings/'+ str(year)] # initialize with the first page's path\n",
    "    else:\n",
    "        webpage_paths = ['/reports/mass-shooting?year='+ str(year)]\n",
    "    for page_number in range(1, number_of_other_pages + 1):\n",
    "        path = re.sub(str(number_of_other_pages), str(page_number), last_webpage_path)\n",
    "        webpage_paths.append(path)\n",
    "    return webpage_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_sources(soup):\n",
    "    news_hrefs = soup.findAll('a', attrs={'href': re.compile(\"^https://|^http://\")})\n",
    "    news_links = [tag.get('href') for tag in news_hrefs if tag.text == \"View Source\"] # get all sources listed on a page\n",
    "    return news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the report tables\n",
    "\n",
    "annual_reports = []\n",
    "for year in range(2014, 2020):\n",
    "    first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    csv_file = str(year) + \"_mass_shootings.csv\"\n",
    "    this_year_report = pd.read_csv(csv_file)\n",
    "    cleaned_report = transform_tables(this_year_report)\n",
    "    annual_reports.append(cleaned_report)\n",
    "\n",
    "ms_2014 = annual_reports[0]\n",
    "ms_2015 = annual_reports[1]\n",
    "ms_2016 = annual_reports[2]\n",
    "ms_2017 = annual_reports[3]\n",
    "ms_2018 = annual_reports[4]\n",
    "ms_2019 = annual_reports[5]\n",
    "\n",
    "ms_2019[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save first pages html\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    if year in range(2014, 2016):\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    else:\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shooting?year=\" + str(year)\n",
    "    path = \"mass_shooting_html_\"+ str(year) \n",
    "    save_html(first_page_url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pages paths\n",
    "\n",
    "web_pages_paths = []\n",
    "for year in range(2014, 2020):\n",
    "    path = \"mass_shooting_html_\"+ str(year)\n",
    "    soup = BeautifulSoup(open(path,'r'), 'html.parser')\n",
    "    web_pages_paths.append(get_webpages(soup, year)) # including the first\n",
    "\n",
    "web_pages_paths[:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_container = np.arange(2014, 2020, 1).tolist()\n",
    "for year_index in range(len(sources_container)):\n",
    "    year = 2014 + year_index\n",
    "    sources_container[year_index] = []\n",
    "    page_index = -1\n",
    "    for path in web_pages_paths[year_index]:\n",
    "        page_index += 1\n",
    "        link = \"https://www.gunviolencearchive.org\" + path\n",
    "        filename = \"mass_shooting_html_\"+ str(year) + \"_page_\" + str(page_index)\n",
    "#         save_html(link, filename)\n",
    "        soup = BeautifulSoup(open(filename,'r'), 'html.parser')\n",
    "        this_page_sources = get_news_sources(soup)\n",
    "        sources_container[year_index].append(this_page_sources) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_container[5][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nesting(nested_list):\n",
    "    return [i for j in nested_list for i in j]\n",
    "\n",
    "news_2014 = remove_nesting(sources_container[0])\n",
    "news_2015 = remove_nesting(sources_container[1])\n",
    "news_2016 = remove_nesting(sources_container[2])\n",
    "news_2017 = remove_nesting(sources_container[3])\n",
    "news_2018 = remove_nesting(sources_container[4])\n",
    "news_2019 = remove_nesting(sources_container[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_2019[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014['Source'] = news_2014\n",
    "ms_2015['Source'] = news_2015\n",
    "ms_2016['Source'] = news_2016\n",
    "ms_2017['Source'] = news_2017\n",
    "ms_2018['Source'] = news_2018\n",
    "\n",
    "# ms_2019['Source'] = news_2019 \n",
    "# gives error since one row does not have a source listed directly\n",
    "\n",
    "index = news_2019.index(\"https://www.wcvb.com/article/6-people-shot-outside-of-roxbury-party-police-say/28306883\") # index of where it is supposed to be \n",
    "news_2019.insert(index, \"https://fox2now.com/2019/07/07/north-county-residents-on-edge-after-5-adults-found-dead-in-apartment/\")\n",
    "news_2019 = [news_2019[i] for i in range(len(news_2019)) if news_2019[i] != news_2019[i-1]] \n",
    "# in case of re-running the insert code and duplicating\n",
    "\n",
    "ms_2019['Source'] = news_2019 \n",
    "ms_2019[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014.to_csv(path_or_buf = \"complete_2014_dataset\")\n",
    "ms_2015.to_csv(path_or_buf = \"complete_2015_dataset\")\n",
    "ms_2016.to_csv(path_or_buf = \"complete_2016_dataset\")\n",
    "ms_2017.to_csv(path_or_buf = \"complete_2017_dataset\")\n",
    "ms_2018.to_csv(path_or_buf = \"complete_2018_dataset\")\n",
    "ms_2019.to_csv(path_or_buf = \"complete_2019_dataset\") # export as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.concat([ms_2014, ms_2015, ms_2016, ms_2017, ms_2018, ms_2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv(path_or_buf = \"complete_project_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
