{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Fingers == Trigger Fingers: A Look at Gun Violence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Cleaning Part 1:\n",
    "Data was gathered from one main source, gunviolencearchive.org. Each individual reporting of gun violence comes with sources to prove that it happened, as well as details on the number injured, killed, date, time, and place. The first data acquisition takes these variables, puts them into a dataframe then into csv, as well as finds the source and cleans the links so that they are easily accessible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import socket\n",
    "import scipy as sc\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tables(pd_dataframe):\n",
    "    # omit the hyperlink column that will be read as NA values\n",
    "    new_dataframe = pd_dataframe.loc[:,\"Incident ID\":\"# Injured\"]\n",
    "    # rename columns\n",
    "    new_dataframe = new_dataframe.rename(columns = {\"Incident ID\": \"ID\", \"Incident Date\": \"Date\", \n",
    "                                    \"State\": \"State\", \"City Or County\": \"City/County\", \n",
    "                                    \"Address\": \"Address\", \"# Killed\": \"Killed\", \n",
    "                                    \"# Injured\": \"Injured\"})\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(url, path):\n",
    "    response = requests.get(url)\n",
    "    with open(path, \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpages(soup, year):\n",
    "    last_webpage_href = soup.find('a', attrs={'title': \"Go to last page\"})\n",
    "    last_webpage_path = last_webpage_href.get('href')\n",
    "    number_of_other_pages = int(re.findall(r'%s(\\d+)'%\"page=\", last_webpage_path)[0])\n",
    "    if year in range(2014, 2016):\n",
    "        webpage_paths = ['/reports/mass-shootings/'+ str(year)] # initialize with the first page's path\n",
    "    else:\n",
    "        webpage_paths = ['/reports/mass-shooting?year='+ str(year)]\n",
    "    for page_number in range(1, number_of_other_pages + 1):\n",
    "        path = re.sub(str(number_of_other_pages), str(page_number), last_webpage_path)\n",
    "        webpage_paths.append(path)\n",
    "    return webpage_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_sources(soup):\n",
    "    news_hrefs = soup.findAll('a', attrs={'href': re.compile(\"^https://|^http://\")})\n",
    "    news_links = [tag.get('href') for tag in news_hrefs if tag.text == \"View Source\"] # get all sources listed on a page\n",
    "    return news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nesting(nested_list):\n",
    "    return [i for j in nested_list for i in j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acquisition and Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pages paths\n",
    "\n",
    "web_pages_paths = []\n",
    "for year in range(2014, 2020):\n",
    "    path = \"mass_shooting_html_\"+ str(year)\n",
    "    soup = BeautifulSoup(open(path,'r'), 'html.parser')\n",
    "    web_pages_paths.append(get_webpages(soup, year)) # including the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save first pages html\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    if year in range(2014, 2016):\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    else:\n",
    "        first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shooting?year=\" + str(year)\n",
    "    path = \"mass_shooting_html_\"+ str(year) \n",
    "    save_html(first_page_url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c9d9a7371b4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"mass_shooting_html_\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_page_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#save_html(link, filename)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mthis_page_sources\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_news_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msources_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0myear_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_page_sources\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"win\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sources_container = np.arange(2014, 2020, 1).tolist()\n",
    "for year_index in range(len(sources_container)):\n",
    "    year = 2014 + year_index\n",
    "    sources_container[year_index] = []\n",
    "    page_index = -1\n",
    "    for path in web_pages_paths[year_index]:\n",
    "        page_index += 1\n",
    "        link = \"https://www.gunviolencearchive.org\" + path\n",
    "        filename = \"mass_shooting_html_\"+ str(year) + \"_page_\" + str(page_index)\n",
    "        #save_html(link, filename)\n",
    "        soup = BeautifulSoup(open(filename,'r'), 'html.parser')\n",
    "        this_page_sources = get_news_sources(soup)\n",
    "        sources_container[year_index].append(this_page_sources) \n",
    "        \n",
    "sources_container[5][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_2014 = remove_nesting(sources_container[0])\n",
    "news_2015 = remove_nesting(sources_container[1])\n",
    "news_2016 = remove_nesting(sources_container[2])\n",
    "news_2017 = remove_nesting(sources_container[3])\n",
    "news_2018 = remove_nesting(sources_container[4])\n",
    "news_2019 = remove_nesting(sources_container[5])\n",
    "news_2019[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the report tables\n",
    "\n",
    "annual_reports = []\n",
    "for year in range(2014, 2020):\n",
    "    first_page_url = \"https://www.gunviolencearchive.org/reports/mass-shootings/\" + str(year)\n",
    "    csv_file = str(year) + \"_mass_shootings.csv\"\n",
    "    this_year_report = pd.read_csv(csv_file)\n",
    "    cleaned_report = transform_tables(this_year_report)\n",
    "    annual_reports.append(cleaned_report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014 = annual_reports[0]\n",
    "ms_2015 = annual_reports[1]\n",
    "ms_2016 = annual_reports[2]\n",
    "ms_2017 = annual_reports[3]\n",
    "ms_2018 = annual_reports[4]\n",
    "ms_2019 = annual_reports[5]\n",
    "\n",
    "ms_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_2014['Source'] = news_2014\n",
    "ms_2015['Source'] = news_2015\n",
    "ms_2016['Source'] = news_2016\n",
    "try:\n",
    "    ms_2017['Source'] = news_2017\n",
    "except Exception as e:\n",
    "    pass\n",
    "ms_2018['Source'] = news_2018\n",
    "\n",
    "# ms_2019['Source'] = news_2019 \n",
    "# gives error since one row does not have a source listed directly\n",
    "\n",
    "index = news_2019.index(\"https://www.wcvb.com/article/6-people-shot-outside-of-roxbury-party-police-say/28306883\") # index of where it is supposed to be \n",
    "news_2019.insert(index, \"https://fox2now.com/2019/07/07/north-county-residents-on-edge-after-5-adults-found-dead-in-apartment/\")\n",
    "news_2019 = [news_2019[i] for i in range(len(news_2019)) if news_2019[i] != news_2019[i-1]] \n",
    "# in case of re-running the insert code and duplicating\n",
    "\n",
    "ms_2019['Source'] = news_2019 \n",
    "ms_2019[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.concat([ms_2014, ms_2015, ms_2016, ms_2017, ms_2018, ms_2019])\n",
    "print(len(merged_data))\n",
    "merged_data.to_csv(path_or_buf = \"complete_project_dataset\") # export as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition Part 2:\n",
    "The second step of the data acquisition process was to access the sources, and find the article text from each source. This allows us to create a second set of data in a text file for analysis of all words from these articles, \"articletext\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads in complete project data\n",
    "gunviolencedataset = pd.read_csv(\"complete_project_dataset\")\n",
    "sourceurl = gunviolencedataset[\"Source\"] #creates a series of just the source urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that takes the source url, goes to the url, reads in the text from that site. The function then writes the text from the \"p\" tag to a text file, so that it can be accessed afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsourcetext(urlseries):\n",
    "    #initialize beautifulsoup\n",
    "    soup = BeautifulSoup('''<html>  </html>''', 'html.parser') \n",
    "    timeout = 20 #creates a timeout variable w an int\n",
    "    socket.setdefaulttimeout(timeout) #uses timeout to set the socket timeout\n",
    "    dictionary = {}\n",
    "    for k in urlseries: #runs this loop for every entry in the series\n",
    "        try:\n",
    "            html = urllib.request.urlopen(url=k) #opens k website\n",
    "            html = html.read() #reads in website info\n",
    "            htmlfile = html.decode('utf-8') #decodes the info into a new file\n",
    "        #need to make exceptions for 404/403/etc\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        else: #what to do after try block works or doesn't work\n",
    "            singlesoup = BeautifulSoup(htmlfile, 'html.parser') #the variable that holds the data from the article\n",
    "            full_text = \"\"\n",
    "            for n in singlesoup(\"p\"): #finds p tag (the main paragraph of the article)\n",
    "                full_text += n.get_text(strip=True)#we only want the parts of the article that are from the main paragraph\n",
    "        dictionary[k] = full_text  \n",
    "    return dictionary\n",
    "\n",
    "text_dict_of_1000 = getsourcetext(sourceurl[:1000]) # it takes 1 hour to work the first 1000 urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized dataframe and save as csv file\n",
    "full_text_df = pd.DataFrame(list(text_dict_of_1000.items()),\n",
    "                   columns=['Source', 'Text'])\n",
    "merged_1000_df = pd.merge(gunviolencedataset, full_text_df, on = \"Source\")\n",
    "merged_1000_df.to_csv(path_or_buf = \"first_1000_dataset_with_text\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the final version of the data by running this code below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the created csv file above\n",
    "imported_df = pd.read_csv(\"first_1000_dataset_with_text\")\n",
    "imported_df['Date'] = imported_df['Date'].apply(lambda x: datetime.datetime.strptime(x,\"%B %d, %Y\"))\n",
    "imported_df = imported_df.sort_values(by = 'Date')\n",
    "no_na = imported_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually inspection show text with 300 or less characters are not article body\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "n = 0\n",
    "n1 = 0\n",
    "m = []\n",
    "s = 'string'\n",
    "textseries = no_na[\"Text\"]\n",
    "eliminationlist = [\"©\", \"Terms of Use\", \"theTerms\", \"Terms of Service\", \"Privacy Policy\", \"JavaScript\", \"Policy•CitizensNet\"]\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "indexlist = no_na.index\n",
    "for string in no_na[\"Text\"]:\n",
    "    i = indexlist[n]\n",
    "    i1 = no_na.loc[i].name\n",
    "    if len(string) < 300:\n",
    "        string = tokenizer.tokenize(string)\n",
    "        for word in string:\n",
    "            if (word in eliminationlist):\n",
    "                m.append(i)\n",
    "                break\n",
    "        n1 += 1    \n",
    "    n += 1\n",
    "print(m), print(n), print(n1)\n",
    "cleaned_imported_df = no_na.drop(labels=m, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = no_na.loc[no_na[\"Text\"].str.len() > 300]\n",
    "data = data.iloc[:, 2:11]\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scraping from the sources of the first 1000 incidents sorted in chronological order, we have all a total of 523 rows. This is due to various reason pertaining to web update: \n",
    "- urls did not work gives 404 status code\n",
    "- article no longer exist\n",
    "- video content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis:\n",
    "After obtaining and cleaning the data, we wanted to look at the statistics and visualization of the data gathered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "num_killed = []\n",
    "num_injured = []\n",
    "\n",
    "for row in range(len(data)):\n",
    "    x = data.iloc[row,1]\n",
    "    if x in dates:\n",
    "        num_killed[len(num_killed)-1]+= data.iloc[row,5]\n",
    "        num_injured[len(num_injured)-1]+= data.iloc[row,6]\n",
    "    else:\n",
    "        dates.append(x)\n",
    "        num_killed.append(data.iloc[row,5])\n",
    "        num_injured.append(data.iloc[row,6])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dates, num_injured, label = \"People Injured\")\n",
    "plt.plot(dates, num_killed, label = \"People Killed\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Dates')\n",
    "# naming the y axis\n",
    "plt.ylabel('Number of People')\n",
    "# giving a title to my graph\n",
    "plt.title('Number of People Killed or Injured by Guns')\n",
    "  \n",
    "# show a legend on the plot\n",
    "plt.legend()\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'statistic' : ['mean','median','mode','std','min','max'], \n",
    "     'Number Killed' : [np.mean(num_killed),np.median(num_killed),statistics.mode(num_killed),np.std(num_killed),min(num_killed),max(num_killed)],\n",
    "    'Number Injured' : [np.mean(num_injured),np.median(num_injured),statistics.mode(num_injured),np.std(num_injured),min(num_injured),max(num_injured)]\n",
    "    }\n",
    "\n",
    "stats_of_the_data = pd.DataFrame(data = d)\n",
    "print(stats_of_the_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data[\"Text\"].str.split()\n",
    "words = words.map(lambda x: len(x))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].plot(data[\"Date\"], words)\n",
    "axs[0].set_title(\"Length of Article by Date\")\n",
    "axs[1].hist(words, bins = 50)\n",
    "axs[1].set_title(\"Length of Article Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the article seems to correlate with the trend in the number of injured or killed victims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob # simple library; sutiable for exploratory analysis\n",
    "def polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity \n",
    "\n",
    "polarity_score = data['Text'].apply(lambda x : polarity(x))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axs[0].plot(data[\"Date\"], polarity_score)\n",
    "axs[0].set_title(\"Polarity Score by Date\")\n",
    "axs[1].hist(polarity_score)\n",
    "axs[1].set_title(\"Polarity Score Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polarity score distribution determined by TextBlob.sentiment.polarity is centered at 0 with a small deviation. The articles are mostly neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Method Outlines: N-grams and Sentiment Analysis Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP Analysis:** We wanted to use Natural Language Processing to inspect the relations of words in the articles we find, in the hopes that we can relate fequency of word use and the context that these fequently used words are in to the increase of gun violence in America. To start this the article text extracted must be tokenized (sorted into words), remove the words and punctuation that are too common to be useful, and assess the frequency of the cleaned words. Then ngrams are created of these words to assess the context that the frequent words come in. \n",
    "\n",
    "What remains to be done is connect these results to collective meaning about the articles, and how that relates to gun violence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import codecs\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# make list of stopwords, numbers, and punctuation\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "capstopwords = [w.title() for w in stopwords]\n",
    "numbers = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"zero\", \",\", \"'\"]\n",
    "stopwords.extend(capstopwords)\n",
    "stopwords.extend(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywordstuple_list = []\n",
    "# for article in data[\"Text\"]:\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     articletext = tokenizer.tokenize(article)\n",
    "#     articletext = [word.lower() for word in articletext if word not in stopwords]\n",
    "#     frequency_dist = FreqDist(articletext)\n",
    "#     keywordstuple = frequency_dist.most_common(10)\n",
    "#     keywordstuple_list.append(list(keywordstuple))\n",
    "\n",
    "# keywordstuple_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncouplevector(vector):\n",
    "    uncoupledvector = []\n",
    "    for ftuple in vector:\n",
    "        t1 = ftuple[0]\n",
    "        t2 = int(ftuple[1])\n",
    "        uncoupledvector.append(t1)\n",
    "        uncoupledvector.append(t2)\n",
    "    return uncoupledvector\n",
    "\n",
    "def averageuncv(vector, N):\n",
    "    n = 1\n",
    "    for element in vector:\n",
    "        if type(element) == type(0):\n",
    "            element = element/N\n",
    "            vector[n] = element\n",
    "            n += 2\n",
    "    return vector\n",
    "\n",
    "def addtokeyworddf(vector, row):\n",
    "    keyrow = keywordDF.iloc[row]\n",
    "    n = 0\n",
    "    for element in vector:\n",
    "        keyrow[n] = element\n",
    "        n += 1\n",
    "    keywordDF.iloc[row] = keyrow\n",
    "    \n",
    "\n",
    "def bigramofDF(dataframe, column):\n",
    "    bigramtuples = []\n",
    "    n=0\n",
    "    bigramdict={}\n",
    "    textdata = dataframe[str(column)]\n",
    "    tokenizer = RegexpTokenizer('/^[a-zA-Z ]*$/', gaps=True)\n",
    "    for row in textdata:\n",
    "        tokenizedarticle = tokenizer.tokenize(row)\n",
    "        tokenizedarticle = [word.lower() for word in tokenizedarticle if word not in stopwords]\n",
    "        bigrams = list(nltk.bigrams(tokenizedarticle))\n",
    "        bigramfreq = FreqDist(bigrams)\n",
    "        top10bigrams = FreqDist(bigrams).most_common(10)\n",
    "        bigramN = bigramfreq.N()\n",
    "        #adds to a 542/20 database of the most frequent bigrams\n",
    "        addtobigramdf(top10bigrams, n, bigramN)\n",
    "        \n",
    "        article = \"article\" + str(n)\n",
    "        bigrams.extend(article)\n",
    "        article_bigrams = tuple(bigrams)\n",
    "        #bigramtuples.append(article_bigrams)\n",
    "        #score = bigrams.score_ngrams(bgm.likelihood_ratio)\n",
    "        #likelihoodscores.append(score)\n",
    "        print(article_bigrams)\n",
    "        n +=1 \n",
    "    return bigramtuples\n",
    "\n",
    "def addtobigramdf(vector, row, N):\n",
    "    keyrow = bigramDF.iloc[row]\n",
    "    n = 0\n",
    "    for element in vector:\n",
    "        keyrow[n] = element[0]\n",
    "        n+=1\n",
    "        keyrow[n] = element[1]/N\n",
    "        n+=1\n",
    "    bigramDF.iloc[row] = keyrow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-058b99c29c15>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  keyrow[n] = element\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1          2         3            4         5   \\\n",
      "0          erie  0.036697     police  0.036697          man  0.027523   \n",
      "1     shootings  0.034483       shot  0.034483       police  0.025862   \n",
      "2      shooting  0.034653       club  0.024752       police  0.014851   \n",
      "3    wednesday,  0.025000       cook  0.025000    mcmillian  0.025000   \n",
      "4        people  0.015464        los  0.015464      angeles  0.015464   \n",
      "..          ...       ...        ...       ...          ...       ...   \n",
      "537   2021march  0.022599     police  0.016949     cockrell  0.016949   \n",
      "538     victims  0.030303       said  0.030303     shooting  0.018182   \n",
      "539      county  0.014493  sheriff’s  0.014493       office  0.014493   \n",
      "540         man  0.020101     police  0.020101     suffered  0.015075   \n",
      "541        said  0.022044     family  0.020040  gramiccioni  0.016032   \n",
      "\n",
      "            6         7            8         9            10        11  \\\n",
      "0     shooting  0.027523        hamot  0.027523         a.m.  0.018349   \n",
      "1           ms  0.017241            -  0.017241      jackson  0.017241   \n",
      "2     arrested  0.014851  benchwarmer  0.014851      suspect  0.014851   \n",
      "3      charged  0.016667         ford  0.016667      heights  0.016667   \n",
      "4      tuesday  0.015464         u.s.  0.015464     shooting  0.010309   \n",
      "..         ...       ...          ...       ...          ...       ...   \n",
      "537  vicksburg  0.011299       l.d.’s  0.011299        found  0.011299   \n",
      "538  northwest  0.009091       police  0.009091   miami-dade  0.009091   \n",
      "539   deputies  0.012422       police  0.010352      officer  0.010352   \n",
      "540    gunshot  0.015075          men  0.010050          bar  0.010050   \n",
      "541       long  0.010020     brittany  0.010020  16-year-old  0.008016   \n",
      "\n",
      "              12        13           14        15       16        17  \\\n",
      "0         sunday  0.018349      morning  0.018349     east  0.018349   \n",
      "1       saturday  0.017241         near  0.017241    m-bar  0.017241   \n",
      "2    15-year-old  0.009901       people  0.009901   sports  0.009901   \n",
      "3        dispute  0.016667       county  0.016667  chicago  0.016667   \n",
      "4      according  0.010309       police  0.010309    media  0.010309   \n",
      "..           ...       ...          ...       ...      ...       ...   \n",
      "537   greenville  0.011299          new  0.011299    route  0.011299   \n",
      "538    shooting,  0.009091  miami-dade,  0.006061   people  0.006061   \n",
      "539         shot  0.008282    apartment  0.008282  douglas  0.008282   \n",
      "540         west  0.010050        north  0.010050  another  0.010050   \n",
      "541        first  0.008016        court  0.008016  kologi,  0.008016   \n",
      "\n",
      "            18        19  \n",
      "0      private  0.018349  \n",
      "1         said  0.017241  \n",
      "2        early  0.009901  \n",
      "3    according  0.016667  \n",
      "4      victims  0.010309  \n",
      "..         ...       ...  \n",
      "537      right  0.011299  \n",
      "538       shot  0.006061  \n",
      "539     deputy  0.008282  \n",
      "540   multiple  0.010050  \n",
      "541   teenager  0.006012  \n",
      "\n",
      "[542 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "l = cleaned_imported_df.shape\n",
    "keywordDF = np.zeros((l[0], 20))\n",
    "keywordDF = pd.DataFrame(keywordDF)\n",
    "n=0\n",
    "m=0\n",
    "vsum=[]\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "for index, row in cleaned_imported_df.iterrows():\n",
    "    tokenizedarticle = tokenizer.tokenize(row[\"Text\"])\n",
    "    articletext = [word.lower() for word in tokenizedarticle if word not in stopwords]\n",
    "    frequencydist = FreqDist(articletext)\n",
    "    wordsN = frequencydist.N()\n",
    "    keywords = frequencydist.most_common(10)\n",
    "    keywordvector = uncouplevector(keywords)\n",
    "    averagedkeywords = averageuncv(keywordvector, wordsN)\n",
    "    addtokeyworddf(averagedkeywords, n)\n",
    "    n += 1\n",
    "print(keywordDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "police        62\n",
       "shooting      38\n",
       "said          27\n",
       "shot          20\n",
       "people        13\n",
       "              ..\n",
       "sheriff's      1\n",
       "medical        1\n",
       "paramedics     1\n",
       "sheriff’s      1\n",
       "center         1\n",
       "Name: 2, Length: 268, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstmostcommonwords = keywordDF[0]\n",
    "firstmostcommonwords.value_counts()\n",
    "secondmostcommondwords = keywordDF[2]\n",
    "secondmostcommondwords.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a graph/track the most common words (5) in article over time(by month?), what do we see(?)\n",
    "#create a graph/ look at/for time lag. Does a word's increase over the course of a few days/weeks (n=1,2,3,4,5,6,7) \n",
    "# do we need more data to fill in data enough for the time lag analysis? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '6', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '7', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '8', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '9', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '0', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '1', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '2', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '3', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '4', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '5', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '6', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '7', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '8', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '1', '9', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '0', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '1', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '2', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '3', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '4', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '5', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '6', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '7', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '8', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '2', '9', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '0', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '1', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '2', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '3', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '4', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '5', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '6', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '7', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '8', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '3', '9', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '0', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '1', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '2', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '3', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '4', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '5', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '6', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '7', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '8', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '4', '9', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '0', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '1', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '2', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '1')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '2')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '3')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '4')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '5')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '6')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '7')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '8')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '3', '9')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '4', '0')\n",
      "('a', 'r', 't', 'i', 'c', 'l', 'e', '5', '4', '1')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-3f47e0a37b6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mbigramdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigramofDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_imported_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mbigramdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import apply_features, accuracy as eval_accuracy\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import (\n",
    "    BigramAssocMeasures,\n",
    "    precision as eval_precision,\n",
    "    recall as eval_recall,\n",
    "    f_measure as eval_f_measure,\n",
    ")\n",
    "bgm    = nltk.collocations.BigramAssocMeasures()\n",
    "bigramDF = np.zeros((l[0], 20))\n",
    "bigramDF = pd.DataFrame(bigramDF)\n",
    "from nltk.probability import FreqDist\n",
    "#ngram analysis of text\n",
    "likelihoodscores = []\n",
    "tuplelist = []\n",
    "\n",
    "    #create a dictionary, key is article#, value is all bigrams\n",
    "\n",
    "bigramdict = bigramofDF(cleaned_imported_df, \"Text\")\n",
    "bigramdict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add death/injury data to article data\n",
    "cleaned_imported_df\n",
    "index1 = cleaned_imported_df.index\n",
    "labels = ['ID', 'Date', 'State', 'City/County', 'Address', 'Killed', 'Injured']\n",
    "interimdf = cleaned_imported_df[labels]\n",
    "#unigram frequencies\n",
    "keywordlabels = [\"1stMCW\", \"1st MCW Freq\", \"2nd MCW\", \"2nd MCW Freq\", \"3rd MCW\", \"3rd MCW Freq\", \"4th MCW\", \"4th MCW Freq\", \"5th MCW\", \"5th MCW Freq\", \"6th MCW\", \"6th MCW Freq\", \"7th MCW\", \"7th MCW Freq\", \"8th MCW\", \"8th MCW Freq\", \"9th MCW\", \"9th MCW Freq\", \"10th MCW\", \"10th MCW Freq\"]\n",
    "keywordDF.columns = keywordlabels\n",
    "keywordDF.index = index1\n",
    "unigramfreq = pd.concat([interimdf, keywordDF], axis=1)\n",
    "unigramfreq.to_csv(\"UnigramFrequencies\")\n",
    "#bigram frequencies\n",
    "keywordlabels = [\"1st MFB\", \"1st MFB Freq\", \"2nd MFB\", \"2nd MFB Freq\", \"3rd MFB\", \"3rd MFB Freq\", \"4th MFB\", \"4th MFB Freq\", \"5th MFB\", \"5th MFB Freq\", \"6th MFB\", \"6th MFB Freq\", \"7th MFB\", \"7th MFB Freq\", \"8th MFB\", \"8th MFB Freq\", \"9th MFB\", \"9th MFB Freq\", \"10th MFB\", \"10th MFB Freq\"]\n",
    "bigramDF.columns = keywordlabels\n",
    "bigramDF.index = index1\n",
    "bigramfreq = pd.concat([bigramDF, interimdf], axis=1) \n",
    "bigramfreq.to_csv(\"BigramFrequencies\")\n",
    "#need to organize by date\n",
    "grouplabel = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dategroupedunigram = unigramfreq.groupby(\"Date\")\n",
    "namelist=[]\n",
    "for name,group in dategroupdunigram:\n",
    "    n1\n",
    "    namelist.append(n1)\n",
    "dategroupedunigram = unigramfreq.aggregate(by=nseries)\n",
    "timestamp = \" 00:00:00\" #add this to index to access data, date is format 2000-01-31\n",
    "#groupedunigram = np.zeros([374,27], dtype=int)\n",
    "#unigrambydate = pd.DataFrame(unigrambydate)\n",
    "nseries = pd.DataFrame(np.zeros([374,1]))\n",
    "print(dategroupedunigram)\n",
    "#unigrambydate = pd.DataFrame(dategroupedunigram)\n",
    "#unigrambydate = unigrambydate[1][1].unstack()\n",
    "#i = unigrambydate.index\n",
    "#unigrambydate['ID', 119]\n",
    "#groupedunigram[0] = unigrambydate[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigramDF[\"1st MFB\"].value_counts())\n",
    "#print(bigramDF[\"2nd MFB\"].value_counts())\n",
    "#print(bigramDF[4].value_counts())\n",
    "#print(bigramDF[6].value_counts())\n",
    "#print(bigramDF[10].value_counts())\n",
    "#print(bigramDF[12].value_counts())\n",
    "#print(bigramDF[14].value_counts())\n",
    "#print(bigramDF[16].value_counts())\n",
    "#print(bigramDF[18].value_counts())\n",
    "grouplabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have most frequent word count and how often it appears in its text, for uni and bi gram. I want to do tri-gram, might not be necessary so I'll do it later. \n",
    "\n",
    "Does the frequency of certain words go up or down by time? Does rates of death/injury correlate to the frequency changes? I think this is a multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "PatsyError",
     "evalue": "Number of rows mismatch between data argument and '1st MCW Freq' (542 versus 1)\n    Killed ~ '1st MCW Freq'\n             ^^^^^^^^^^^^^^",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-462d81237b90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msinglewordfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"UnigramFrequencies\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0munigramols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Killed ~ '1st MCW Freq'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msinglewordfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0munigramols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[1;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         tmp = handle_formula_data(data, None, formula, depth=eval_env,\n\u001b[0m\u001b[0;32m    170\u001b[0m                                   missing=missing)\n\u001b[0;32m    171\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesign_info\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\formula\\formulatools.py\u001b[0m in \u001b[0;36mhandle_formula_data\u001b[1;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_using_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n\u001b[0m\u001b[0;32m     64\u001b[0m                                NA_action=na_action)\n\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\highlevel.py\u001b[0m in \u001b[0;36mdmatrices\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \"\"\"\n\u001b[0;32m    308\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[0m\u001b[0;32m    310\u001b[0m                                       NA_action, return_type)\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\highlevel.py\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    165\u001b[0m                                       NA_action)\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdesign_infos\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         return build_design_matrices(design_infos, data,\n\u001b[0m\u001b[0;32m    168\u001b[0m                                      \u001b[0mNA_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNA_action\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                                      return_type=return_type)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\build.py\u001b[0m in \u001b[0;36mbuild_design_matrices\u001b[1;34m(design_infos, data, NA_action, return_type, dtype)\u001b[0m\n\u001b[0;32m    891\u001b[0m                 \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactor_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactor_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                 \u001b[0mrows_checker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m                 if (have_pandas\n\u001b[0;32m    895\u001b[0m                     and isinstance(value, (pandas.Series, pandas.DataFrame))):\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\build.py\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, seen_value, desc, origin)\u001b[0m\n\u001b[0;32m    793\u001b[0m                 \u001b[1;31m# XX FIXME: this is a case where having discontiguous Origins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                 \u001b[1;31m# would be useful...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mPatsyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m def build_design_matrices(design_infos, data,\n",
      "\u001b[1;31mPatsyError\u001b[0m: Number of rows mismatch between data argument and '1st MCW Freq' (542 versus 1)\n    Killed ~ '1st MCW Freq'\n             ^^^^^^^^^^^^^^"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "singlewordfreq = pd.read_csv(\"UnigramFrequencies\")\n",
    "unigramols = sm.ols(formula=\"Killed ~ '1st MCW Freq'\", data=singlewordfreq).fit()\n",
    "unigramols.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Analysis:** Our second kind of analysis for the article is going to be sentiment analysis classification. We created a classification model that trains on a library of tweets that are rated from 0 through 4 on a negative-positive scale. The tweet is vectorized into features and its rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "except Exception as e:\n",
    "    pass\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and trimming twitter sentiment dataset\n",
    "\n",
    "Insert header, remove numbers, usernames, and NO_QUERY\n",
    "\n",
    "Move first row that became header down to data, add headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding = \"ISO-8859-1\",names=[\"score\",\"id\",\"datetime\",\"NO_QUERY\",\"usernames\",\"tweet\"])\n",
    "\n",
    "df = df.drop(['id','NO_QUERY','usernames'],axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking count values, dataset listed from 0 = negative to 4 = positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0:20000, :]\n",
    "print(df.shape)\n",
    "print(df['score'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up links, @users, hastags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def cleanup(text):\n",
    "    clean = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(text)).split()\n",
    "    tokens = []\n",
    "    for token in clean:\n",
    "        if token not in stopwords:\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'] = df['tweet'].apply(lambda x: cleanup(x))\n",
    "\n",
    "df['score'] = df['score'].replace([0,4],['neg','pos']) \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuple(x):\n",
    "    subset = x[['tweet','score']]\n",
    "    tuples = [tuple(i) for i in subset.to_numpy()]\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = to_tuple(df)\n",
    "documents[0] # tuple of tokens and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_master(x):\n",
    "#     master_list = []\n",
    "#     for i in range(len(x)):\n",
    "#         master_list += x['tweet'][i] \n",
    "#     return master_list\n",
    "\n",
    "words = remove_nesting(df['tweet'])\n",
    "\n",
    "word_features = nltk.FreqDist(w.lower() for w in words).most_common(2000)\n",
    "word_features =  [word_tuple[0] for word_tuple in word_features]\n",
    "word_features[0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):    \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains('+ word +')'] = (word in document_words) \n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents] \n",
    "featuresets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words as feature vectors have been computed, this then establishes the training and test sets. Then runs the classification model on the test set. \n",
    "\n",
    "What remains to be done here is test the model on the article data which also needs to be turned into feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = []\n",
    "for article in data[\"Text\"]:\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    articletext = tokenizer.tokenize(article)\n",
    "    articletext = [word.lower() for word in articletext if word not in stopwords]\n",
    "    text_labels.append(classifier.classify(document_features(article)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(text_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gun violence data is collected manually from reliable news and police reports in an organized manner and is intended for public use. Since we are not republishing any material or using any metadata, there is also no conflict with the Rights and Limit of Use stated by the aforementioned news outlets.\n",
    "\n",
    "There could be some unintended bias in the data collection: all incidents with dysfunctional links were ommitted. It would be very time-consuming and ineffective to find substitute urls otherwise. \n",
    "\n",
    "We are working with statistics surrounding violent death or injury, it must be remembered that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
